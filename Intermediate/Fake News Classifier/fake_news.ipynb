{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PhxTkEHPZWv"
   },
   "source": [
    "Develop a machine learning or deep learning program to identify when an article might be fake news. \n",
    "Download the dataset here: https://www.kaggle.com/c/fake-news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "StqygOZzzYoJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/train.csv')\n",
    "df = df.dropna()\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "eUodRCh3zn_0",
    "outputId": "b9092937-5473-450b-f884-c19e2b712d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xD-ca5qyzykN"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "text = X.copy()\n",
    "text.reset_index(inplace=True)\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(0, len(text)):\n",
    "  cleaning = re.sub('[^a-zA-Z]', ' ', text['title'][i])\n",
    "  cleaning = cleaning.lower()\n",
    "  cleaning = cleaning.split()\n",
    "\n",
    "  cleaning = [ps.stem(word) for word in cleaning if not word in stopwords.words('english')]\n",
    "  cleaning = \" \".join(cleaning)\n",
    "  corpus.append(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oZIP8I1X0Pe2"
   },
   "outputs": [],
   "source": [
    "voc_size = 5000 #Number of words for the one hot encoding\n",
    "sent_length = 20 #Max length for padding\n",
    "embedding_vector_features = 40 #Number of vector features for embedding\n",
    "\n",
    "#One hot encoding\n",
    "onehot_repr = [one_hot(sentence, voc_size) for sentence in corpus]\n",
    "\n",
    "#Padding\n",
    "embedded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "T82wFoBL0mvg",
    "outputId": "ad9e5065-ac89-4724-c2f0-caa5a4eb4d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 40)            200000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 100)           56400     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 20, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 100)           80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 418,501\n",
      "Trainable params: 417,901\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_vector_features, input_length=sent_length))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "xeH4kXdm0wjo",
    "outputId": "1731cd99-9ce4-4fd4-afa0-cc725bbd7072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147/147 [==============================] - 3s 18ms/step - loss: 0.2795 - accuracy: 0.8823 - val_loss: 0.9002 - val_accuracy: 0.4397\n",
      "Epoch 2/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.1627 - accuracy: 0.9328 - val_loss: 0.2656 - val_accuracy: 0.8953\n",
      "Epoch 3/20\n",
      "147/147 [==============================] - 2s 13ms/step - loss: 0.1222 - accuracy: 0.9536 - val_loss: 0.2012 - val_accuracy: 0.9155\n",
      "Epoch 4/20\n",
      "147/147 [==============================] - 2s 13ms/step - loss: 0.1079 - accuracy: 0.9595 - val_loss: 0.2489 - val_accuracy: 0.9152\n",
      "Epoch 5/20\n",
      "147/147 [==============================] - 2s 13ms/step - loss: 0.0966 - accuracy: 0.9653 - val_loss: 0.2679 - val_accuracy: 0.9136\n",
      "Epoch 6/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0816 - accuracy: 0.9712 - val_loss: 0.3336 - val_accuracy: 0.9062\n",
      "Epoch 7/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0619 - accuracy: 0.9777 - val_loss: 0.2656 - val_accuracy: 0.9158\n",
      "Epoch 8/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0604 - accuracy: 0.9796 - val_loss: 0.3092 - val_accuracy: 0.9177\n",
      "Epoch 9/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0584 - accuracy: 0.9807 - val_loss: 0.3062 - val_accuracy: 0.9215\n",
      "Epoch 10/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0529 - accuracy: 0.9838 - val_loss: 0.2893 - val_accuracy: 0.9185\n",
      "Epoch 11/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 0.3270 - val_accuracy: 0.9098\n",
      "Epoch 12/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.3609 - val_accuracy: 0.9212\n",
      "Epoch 13/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0512 - accuracy: 0.9820 - val_loss: 0.2983 - val_accuracy: 0.9232\n",
      "Epoch 14/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0560 - accuracy: 0.9822 - val_loss: 0.3555 - val_accuracy: 0.9163\n",
      "Epoch 15/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0462 - accuracy: 0.9845 - val_loss: 0.3407 - val_accuracy: 0.9185\n",
      "Epoch 16/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0485 - accuracy: 0.9845 - val_loss: 0.3321 - val_accuracy: 0.9122\n",
      "Epoch 17/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0508 - accuracy: 0.9828 - val_loss: 0.3235 - val_accuracy: 0.9111\n",
      "Epoch 18/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0558 - accuracy: 0.9826 - val_loss: 0.3565 - val_accuracy: 0.9158\n",
      "Epoch 19/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0506 - accuracy: 0.9843 - val_loss: 0.3309 - val_accuracy: 0.9180\n",
      "Epoch 20/20\n",
      "147/147 [==============================] - 2s 12ms/step - loss: 0.0451 - accuracy: 0.9859 - val_loss: 0.3613 - val_accuracy: 0.9232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa9100f9080>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to numpy array\n",
    "\n",
    "X_final = np.array(embedded_docs)\n",
    "\n",
    "y_final = np.array(y)\n",
    "\n",
    "#Splitting dataset to training and testing \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=77)\n",
    "\n",
    "#Model training\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "QxABt2Z100Zc",
    "outputId": "87afa68c-7e55-46a7-d463-86ac6b0b000f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-bf99ef7a3e92>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[[1902  164]\n",
      " [ 117 1474]]\n",
      "0.9231610609789445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      2066\n",
      "           1       0.90      0.93      0.91      1591\n",
      "\n",
      "    accuracy                           0.92      3657\n",
      "   macro avg       0.92      0.92      0.92      3657\n",
      "weighted avg       0.92      0.92      0.92      3657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78-Q3aPp1tKc"
   },
   "source": [
    "The LSTM model used here gained an accuracy of over 93% after the submission in kaggle. You could try implementing transfer learning, experimenting with different hyperparameters or get some more data to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
